
<!-- saved from url=(0037)http://cnnlocalization.csail.mit.edu/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<title>MALL-code</title>

<script async="" src="./mall-iitd/analytics.js.download"></script><script type="text/javascript" src="./mall-iitd/jquery.mlens-1.0.min.js.download"></script>
<script type="text/javascript" src="./mall-iitd/jquery.js.download"></script>
<style>
body
{
    font-family : Arial;
	background-color : #111;
}
.content
{
    width : 800px;
    padding : 25px 50px;
    margin : 25px auto;
    background-color : #fff;
    box-shadow: 0px 0px 10px #999;
    border-radius: 15px; 
}

.contentblock
{
    width : 950px;
    margin : 0 auto;
    padding : 0;
    border-spacing : 25px 0;
}

.contentblock td
{
    background-color : #fff;
    padding : 25px 50px;
    vertical-align : top;
    box-shadow: 0px 0px 10px #999;
    border-radius: 15px; 
}

a, a:visited
{
    color: #224b8d;
}

#authors
{
    text-align : center;
    margin-bottom : 20px;
}

#conference
{
    text-align : center;
    margin-bottom : 20px;
    font-style : italic;
}

#authors a 
{
    margin : 0 10px;
}

h1
{
    text-align : center;
    font-family : Arial;
    font-size : 20px;
}

code
{
	display : block;
	padding : 10px;
	margin : 10px 10px;
}
p code
{
    display : inline;
    padding : 0;
    margin : 0;
}
#teasers
{
    margin : 0 auto;    
}

#teasers td
{
    margin : 0 auto;
    text-align : center;
    padding : 5px;
}

#teasers img
{
    width : 250px; 
}

#results img
{
    width : 133px;
}

#seeintodark {
    margin : 0 auto;
}

#sift 
{
    margin : 0 auto;
}

#sift img
{
    width : 250px;
}

.downloadpaper 
{
    padding-left : 20px;
    float : right;
    text-align : center;
}

.downloadpaper a 
{
    font-weight : bold;
    text-align : center;
}

#demoframe
{
    border : 0;
    padding : 0;
    margin : 0;
    width : 100%;
    height : 340px;
}

#feedbackform
{
    border : 1px solid #ccc;
    margin : 0 auto;
    border-radius : 15px;
}

#eyeglass {
    height : 530px;
}

#eyeglass #wrapper {
    position: relative;
    height: auto;
    margin: 0 auto;
    float: left;
    width : 800px;
}

#mitnews
{ 
    font-weight : normal;
    margin-top : 20px;
    font-size : 12px;
    width : 220px;
}

#mitnews a {
    font-weight : normal;
}
</style>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-23931362-6', 'auto');
  ga('send', 'pageview');

</script>

</head>

<body data-new-gr-c-s-check-loaded="14.1039.0" data-gr-ext-installed="">

<div class="content">

<h1>Master of All: Simultaneous Generalization of Urban-Scene Segmentation to All Adverse Weather Conditions</h1>
<p id="authors">
<a href=mailto:nikhil.jangamreddy@uqidar.iitd.ac.in>Nikhil Reddy</a><sup>a </sup> 
<a href="">Abhinav Singhal</a><sup>a </sup> 
<a href="">Abhishek kumar</a><sup>a </sup> 
<a href="">Mahsa Baktashmotlagh</a><sup>b </sup> 
<a href="">Chetan Arora</a><sup>a </sup> 
<br>
<br>
<sup> a </sup> Indian Institute of Technology, Delhi <br>
<sup> b </sup> The University of Queensland, Australia.
</p>

<div class="downloadpaper">
<a href="./mall-iitd/architechture.pdf"><img src="./mall-iitd/architechture.pdf" width="400px" border="2"></a>
</div>

<p>Computer vision systems for autonomous navigation must generalize well in adverse weather and illumination conditions expected in the real world. However, semantic segmentation of images captured in such conditions remains a challenging task for current state-of-theart (SOTA) methods trained on broad daylight images, due to the associated distribution shift. On the other hand, domain adaptation techniques developed for the purpose rely on the availability of the source data, (un)labeled target data and/or its auxiliary information (e.g., GPS). Even then, they typically adapt to a single(specific) target domain(s). To remedy this, we propose a novel, fully test time, adaptation technique, named Master of ALL (MALL), for simultaneous generalization to multiple target domains. MALL learns to generalize on unseen adverse weather images from multiple target domains directly at the inference time. More specifically, given a pre-trained model and its parameters, MALL enforces edge consistency prior at the inference stage and updates the model based on (a) a single test sample at a time (MALL-sample), or (b) continuously for the whole test domain (MALL-domain). Not only the target data, MALL also does not need access to the source data and thus, can be used with any pre-trained model. Using a simple model pre-trained on daylight images, MALL outperforms specially designed adverse weather semantic segmentation methods, both in domain generalization and testtime adaptation settings. Our experiments on foggy, snow, night, cloudy, overcast, and rainy conditions demonstrate the target domain-agnostic effectiveness of our approach. We further show that MALL can improve the performance of a model on an adverse weather condition, even when the model is already pre-trained for the specific condition.
</p>

<p><a href="./mall-iitd/08360.pdf" target="_blank">Paper</a> &nbsp;&nbsp;&nbsp;&nbsp; <a href="./mall-iitd/08360-supp.pdf" target="_blank">Supplementary</a></p>

<p><a href="https://github.com/mall-code/code-repository" target="_blank">Source code</a></p>

<br clear="all">
</div>







<!--div class="content" id="references">

<h2>Reference</h2>

<p>B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning Deep Features for Discriminative Localization. CVPR'16 (arXiv:1512.04150, 2015).</p>

<code>
@article{zhou2015cnnlocalization,<br>
&nbsp;&nbsp;title={{Learning Deep Features for Discriminative Localization.}},<br>
&nbsp;&nbsp;author={Zhou, B. and Khosla, A. and Lapedriza. A. and Oliva, A. and Torralba, A.},<br>
&nbsp;&nbsp;journal={CVPR},<br>
&nbsp;&nbsp;year={2016}<br>
}
</code>

<p>Acknowledgement: <br>This work was supported by NSF grant IIS-1524817, and by a Google faculty research award to A.T.</p>

<p></p><center><a href="https://accessibility.mit.edu/"><b>Accessibility</b></a></center><p></p>
</div-->

</body>
</html>
